{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/isabellechen/git-repos/tutorial/makemore/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=open('names.txt','r').read().split()\n",
    "\n",
    "b={}\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        bigram = (ch1, ch2)\n",
    "        b[bigram]=b.get(bigram, 0) + 1\n",
    "        #print(ch1,ch2)\n",
    "\n",
    "chars = sorted(set(''.join(words)))\n",
    "char_to_int = {c: i+1 for i, c in enumerate(chars)}\n",
    "int_to_char = {i+1: c for i, c in enumerate(chars)}\n",
    "char_to_int['.']=0\n",
    "int_to_char[0]='.'\n",
    "N = torch.zeros((27,27), dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = [],[]\n",
    "\n",
    "for w in words[:1]:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = char_to_int[ch1]\n",
    "        ix2 = char_to_int[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13, 13,  1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys\n",
    "\n",
    "# For instance, When 5 (i.e. xs[1]) is the input, we want the probability of 13 (i.e. ys[1]) to be high!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hot encodings\n",
    "- We want integers, but we cannot put it directly into NN.\n",
    "- NN is made up of neurons, and neurons are made up of weights and biases.\n",
    "- We shouldn't multiply weights (and biases) by integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "xenc = F.one_hot(xs, num_classes=27)\n",
    "xenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12f484dc0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xenc, aspect='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cast the list of ints to list of floats\n",
    "xenc = xenc.float()\n",
    "xenc.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0045],\n",
       "        [ 0.5301],\n",
       "        [-1.2283],\n",
       "        [-1.2283],\n",
       "        [-0.6871]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RECALL: W dot x + b\n",
    "\n",
    "# Initialize W\n",
    "W = torch.randn((27, 1))\n",
    "xenc @ W # Matrix multiplication function, which produces the log counts\n",
    "\n",
    "# Think about this (5, 27) x (27, 1) = (5, 1): 5 activations for 5 inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3814, -3.2431,  3.2540, -0.1137,  0.2812,  1.1645, -0.2796, -0.5477,\n",
       "          0.4394, -1.0565,  0.2692, -0.9781,  0.8879, -1.1311,  0.9895,  0.1794,\n",
       "          0.1834,  0.7107, -2.6506,  2.4171, -0.2975, -0.8815,  0.0172, -0.9695,\n",
       "          0.5162, -0.3191,  0.2113],\n",
       "        [-0.4322,  1.4631,  0.6997,  0.2058,  0.6570,  0.1313, -0.4899,  0.5045,\n",
       "          1.8324, -0.6401, -0.8893,  0.1733,  1.0635,  1.2387, -0.0138, -1.6044,\n",
       "          2.3672, -0.5829,  0.0129, -0.6739, -0.0633, -0.3612, -1.3129,  0.7221,\n",
       "          1.1962, -0.9146, -2.0454],\n",
       "        [-0.0369,  0.9454,  1.6991,  0.4702, -0.1503, -0.8581, -0.5607, -0.0366,\n",
       "         -0.5037,  0.2984, -0.7716,  1.2557, -2.0261,  0.5080, -0.2577, -1.6995,\n",
       "          0.0199, -0.4689,  0.2402,  1.2434, -0.2129,  0.9037,  0.9149,  1.5517,\n",
       "          0.3690, -0.3650,  0.4971],\n",
       "        [-0.0369,  0.9454,  1.6991,  0.4702, -0.1503, -0.8581, -0.5607, -0.0366,\n",
       "         -0.5037,  0.2984, -0.7716,  1.2557, -2.0261,  0.5080, -0.2577, -1.6995,\n",
       "          0.0199, -0.4689,  0.2402,  1.2434, -0.2129,  0.9037,  0.9149,  1.5517,\n",
       "          0.3690, -0.3650,  0.4971],\n",
       "        [ 1.7980, -0.2138, -1.5245, -1.2308,  2.2870, -1.9768,  0.2640, -0.0574,\n",
       "          0.0964,  0.0569,  0.6542,  0.2129,  1.0586, -1.8088,  0.9807, -0.1509,\n",
       "         -0.9854,  0.2442,  0.1901,  0.0794, -0.4655, -0.4063, -0.9325, -1.2595,\n",
       "          0.5074,  1.0277, -0.1532]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taking this one step further, we can have multiple neurons in the hidden layer\n",
    "\n",
    "W = torch.randn((27, 27)) \n",
    "# the first 27 is the number of input neurons\n",
    "# the second 27 is the number of neuron in the hidden layer\n",
    "logits = xenc @ W\n",
    "\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0589, 0.0006, 0.3830, 0.0132, 0.0196, 0.0474, 0.0112, 0.0086, 0.0230,\n",
       "         0.0051, 0.0194, 0.0056, 0.0359, 0.0048, 0.0398, 0.0177, 0.0178, 0.0301,\n",
       "         0.0010, 0.1659, 0.0110, 0.0061, 0.0150, 0.0056, 0.0248, 0.0108, 0.0183],\n",
       "        [0.0130, 0.0864, 0.0403, 0.0246, 0.0386, 0.0228, 0.0123, 0.0331, 0.1250,\n",
       "         0.0105, 0.0082, 0.0238, 0.0579, 0.0690, 0.0197, 0.0040, 0.2133, 0.0112,\n",
       "         0.0203, 0.0102, 0.0188, 0.0139, 0.0054, 0.0412, 0.0661, 0.0080, 0.0026],\n",
       "        [0.0225, 0.0602, 0.1279, 0.0374, 0.0201, 0.0099, 0.0133, 0.0225, 0.0141,\n",
       "         0.0315, 0.0108, 0.0821, 0.0031, 0.0389, 0.0181, 0.0043, 0.0239, 0.0146,\n",
       "         0.0297, 0.0811, 0.0189, 0.0577, 0.0584, 0.1104, 0.0338, 0.0162, 0.0384],\n",
       "        [0.0225, 0.0602, 0.1279, 0.0374, 0.0201, 0.0099, 0.0133, 0.0225, 0.0141,\n",
       "         0.0315, 0.0108, 0.0821, 0.0031, 0.0389, 0.0181, 0.0043, 0.0239, 0.0146,\n",
       "         0.0297, 0.0811, 0.0189, 0.0577, 0.0584, 0.1104, 0.0338, 0.0162, 0.0384],\n",
       "        [0.1414, 0.0189, 0.0051, 0.0068, 0.2305, 0.0032, 0.0305, 0.0221, 0.0258,\n",
       "         0.0248, 0.0450, 0.0290, 0.0675, 0.0038, 0.0624, 0.0201, 0.0087, 0.0299,\n",
       "         0.0283, 0.0254, 0.0147, 0.0156, 0.0092, 0.0066, 0.0389, 0.0654, 0.0201]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = logits.exp() # Exponentiate the log counts to get the counts\n",
    "probs = counts / counts.sum(1, keepdim=True) # Normalize the counts to get the probabilities\n",
    "probs # The probability of the next character given the current character. The calculation is known as the softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram:.e\n",
      "Input to NN:0\n",
      "Output probability predictions:tensor([0.0589, 0.0006, 0.3830, 0.0132, 0.0196, 0.0474, 0.0112, 0.0086, 0.0230,\n",
      "        0.0051, 0.0194, 0.0056, 0.0359, 0.0048, 0.0398, 0.0177, 0.0178, 0.0301,\n",
      "        0.0010, 0.1659, 0.0110, 0.0061, 0.0150, 0.0056, 0.0248, 0.0108, 0.0183])\n",
      "Acutal:e\n",
      "Probability assigned by the net to the correct charater:0.047397878021001816\n",
      "Negative log likelihood:3.049177885055542\n",
      "----\n",
      "\n",
      "Bigram:em\n",
      "Input to NN:5\n",
      "Output probability predictions:tensor([0.0130, 0.0864, 0.0403, 0.0246, 0.0386, 0.0228, 0.0123, 0.0331, 0.1250,\n",
      "        0.0105, 0.0082, 0.0238, 0.0579, 0.0690, 0.0197, 0.0040, 0.2133, 0.0112,\n",
      "        0.0203, 0.0102, 0.0188, 0.0139, 0.0054, 0.0412, 0.0661, 0.0080, 0.0026])\n",
      "Acutal:m\n",
      "Probability assigned by the net to the correct charater:0.06900893896818161\n",
      "Negative log likelihood:2.6735191345214844\n",
      "----\n",
      "\n",
      "Bigram:mm\n",
      "Input to NN:13\n",
      "Output probability predictions:tensor([0.0225, 0.0602, 0.1279, 0.0374, 0.0201, 0.0099, 0.0133, 0.0225, 0.0141,\n",
      "        0.0315, 0.0108, 0.0821, 0.0031, 0.0389, 0.0181, 0.0043, 0.0239, 0.0146,\n",
      "        0.0297, 0.0811, 0.0189, 0.0577, 0.0584, 0.1104, 0.0338, 0.0162, 0.0384])\n",
      "Acutal:m\n",
      "Probability assigned by the net to the correct charater:0.03886421397328377\n",
      "Negative log likelihood:3.2476813793182373\n",
      "----\n",
      "\n",
      "Bigram:ma\n",
      "Input to NN:13\n",
      "Output probability predictions:tensor([0.0225, 0.0602, 0.1279, 0.0374, 0.0201, 0.0099, 0.0133, 0.0225, 0.0141,\n",
      "        0.0315, 0.0108, 0.0821, 0.0031, 0.0389, 0.0181, 0.0043, 0.0239, 0.0146,\n",
      "        0.0297, 0.0811, 0.0189, 0.0577, 0.0584, 0.1104, 0.0338, 0.0162, 0.0384])\n",
      "Acutal:a\n",
      "Probability assigned by the net to the correct charater:0.06019170209765434\n",
      "Negative log likelihood:2.810220718383789\n",
      "----\n",
      "\n",
      "Bigram:a.\n",
      "Input to NN:1\n",
      "Output probability predictions:tensor([0.1414, 0.0189, 0.0051, 0.0068, 0.2305, 0.0032, 0.0305, 0.0221, 0.0258,\n",
      "        0.0248, 0.0450, 0.0290, 0.0675, 0.0038, 0.0624, 0.0201, 0.0087, 0.0299,\n",
      "        0.0283, 0.0254, 0.0147, 0.0156, 0.0092, 0.0066, 0.0389, 0.0654, 0.0201])\n",
      "Acutal:.\n",
      "Probability assigned by the net to the correct charater:0.14137695729732513\n",
      "Negative log likelihood:1.9563255310058594\n",
      "----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# How the model is doing with random W\n",
    "\n",
    "l = len(words[0])+1\n",
    "nlls = torch.zeros(l)\n",
    "\n",
    "for i in range(l):\n",
    "    # i-th bigram\n",
    "    x=xs[i].item()\n",
    "    y=ys[i].item()\n",
    "\n",
    "    # The probability of the next character given the current character\n",
    "    print(f\"Bigram:{int_to_char[x]}{int_to_char[y]}\")\n",
    "    print(f\"Input to NN:{x}\")\n",
    "    print(f\"Output probability predictions:{probs[i]}\")\n",
    "    print(f\"Acutal:{int_to_char[y]}\")\n",
    "    p = probs[i, y]\n",
    "    print(f\"Probability assigned by the net to the correct charater:{p.item()}\")\n",
    "    nll = -torch.log(p).item()\n",
    "    print(f\"Negative log likelihood:{nll}\")\n",
    "    nlls[i] = nll\n",
    "    print(\"----\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num = xs.nelement()\n",
    "print('number of examples: ', num)\n",
    "\n",
    "# initialize the 'network'\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.778970241546631\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "for k in range(1):\n",
    "  \n",
    "  # forward pass\n",
    "  xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "  logits = xenc @ W # predict log-counts\n",
    "  counts = logits.exp() # counts, equivalent to N\n",
    "  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "  loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean() # Familiar with this? Regularization term!\n",
    "  print(loss.item())\n",
    "  \n",
    "  # backward pass\n",
    "  W.grad = None # set to zero the gradient\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ema.\n",
      "emmmma.\n",
      "ema.\n",
      "ema.\n",
      "emma.\n"
     ]
    }
   ],
   "source": [
    "# finally, sample from the 'neural net' model\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "  \n",
    "  out = []\n",
    "  ix = 0\n",
    "  while True:\n",
    "    \n",
    "    # ----------\n",
    "    # BEFORE:\n",
    "    #p = P[ix]\n",
    "    # ----------\n",
    "    # NOW:\n",
    "    xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "    logits = xenc @ W # predict log-counts\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "    # ----------\n",
    "    \n",
    "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    out.append(int_to_char[ix])\n",
    "    if ix == 0:\n",
    "      break\n",
    "  print(''.join(out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "makemore-OzY8FFiS-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
