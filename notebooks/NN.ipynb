{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/isabellechen/git-repos/tutorial/makemore/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=open('names.txt','r').read().split()\n",
    "\n",
    "b={}\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        bigram = (ch1, ch2)\n",
    "        b[bigram]=b.get(bigram, 0) + 1\n",
    "        #print(ch1,ch2)\n",
    "\n",
    "chars = sorted(set(''.join(words)))\n",
    "char_to_int = {c: i+1 for i, c in enumerate(chars)}\n",
    "int_to_char = {i+1: c for i, c in enumerate(chars)}\n",
    "char_to_int['.']=0\n",
    "int_to_char[0]='.'\n",
    "N = torch.zeros((27,27), dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = [],[]\n",
    "\n",
    "for w in words[:1]:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = char_to_int[ch1]\n",
    "        ix2 = char_to_int[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13, 13,  1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys\n",
    "\n",
    "# For instance, When 5 (i.e. xs[1]) is the input, we want the probability of 13 (i.e. ys[1]) to be high!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hot encodings\n",
    "- We want integers, but we cannot put it directly into NN.\n",
    "- NN is made up of neurons, and neurons are made up of weights and biases.\n",
    "- We shouldn't multiply weights (and biases) by integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "xenc = F.one_hot(xs, num_classes=27)\n",
    "xenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1515e2680>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xenc, aspect='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cast the list of ints to list of floats\n",
    "xenc = xenc.float()\n",
    "xenc.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1522],\n",
       "        [ 0.2123],\n",
       "        [-0.5055],\n",
       "        [-0.5055],\n",
       "        [ 0.7118]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RECALL: W dot x + b\n",
    "\n",
    "# Initialize W\n",
    "W = torch.randn((27, 1))\n",
    "xenc @ W # Matrix multiplication function, which produces the log counts\n",
    "\n",
    "# Think about this (5, 27) x (27, 1) = (5, 1): 5 activations for 5 inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.8709e+00, -1.0462e+00, -4.6835e-01,  2.2907e+00,  5.7680e-01,\n",
       "         -8.7377e-01,  1.4182e+00,  1.3438e+00,  8.8496e-01,  9.0372e-01,\n",
       "          1.6850e+00, -7.8376e-01,  5.9498e-01, -2.8811e-01, -5.0097e-01,\n",
       "         -6.5894e-01, -2.3727e+00, -1.0648e+00, -1.3526e+00,  9.6133e-03,\n",
       "          2.0751e+00, -8.5166e-01, -1.6030e-01, -8.5588e-01, -6.0361e-01,\n",
       "         -3.4536e-01, -1.4265e+00],\n",
       "        [-5.1194e-04,  2.1266e+00, -1.2661e+00,  6.8527e-01, -8.1048e-01,\n",
       "          1.6607e+00,  1.0307e+00, -1.0040e+00,  2.3773e-01,  2.4760e-01,\n",
       "         -1.9350e+00, -2.0920e-01,  2.7899e-02,  6.3108e-01, -4.9587e-01,\n",
       "         -1.1853e-01, -2.3463e-01, -6.0408e-01,  2.3138e+00, -2.0228e+00,\n",
       "         -1.3048e+00, -2.0521e+00,  6.1603e-01,  5.6111e-01, -1.6532e+00,\n",
       "         -1.0335e+00, -1.1863e+00],\n",
       "        [-4.8596e-01, -4.2202e-01, -6.7855e-01, -7.7467e-01,  1.4107e+00,\n",
       "         -2.0777e+00, -1.2289e+00,  2.0390e+00,  1.5057e+00,  3.0532e-01,\n",
       "         -6.5983e-01,  3.5799e-01,  1.1949e+00, -6.6051e-01, -9.0597e-01,\n",
       "          2.3551e+00, -7.4628e-01,  2.0861e-01,  3.2167e-01,  1.0338e+00,\n",
       "          4.0889e-01, -1.1365e+00, -8.6047e-01,  2.4210e-01, -1.1335e+00,\n",
       "         -1.1735e-02, -3.1684e-01],\n",
       "        [-4.8596e-01, -4.2202e-01, -6.7855e-01, -7.7467e-01,  1.4107e+00,\n",
       "         -2.0777e+00, -1.2289e+00,  2.0390e+00,  1.5057e+00,  3.0532e-01,\n",
       "         -6.5983e-01,  3.5799e-01,  1.1949e+00, -6.6051e-01, -9.0597e-01,\n",
       "          2.3551e+00, -7.4628e-01,  2.0861e-01,  3.2167e-01,  1.0338e+00,\n",
       "          4.0889e-01, -1.1365e+00, -8.6047e-01,  2.4210e-01, -1.1335e+00,\n",
       "         -1.1735e-02, -3.1684e-01],\n",
       "        [-1.0715e+00,  1.5543e+00,  9.9366e-01, -9.8725e-01,  2.6984e-01,\n",
       "         -1.0728e+00,  1.1626e+00, -4.6379e-01,  5.6711e-01, -1.1041e+00,\n",
       "          9.0720e-01, -3.5498e-01,  5.9670e-01,  1.7660e-01,  1.1893e+00,\n",
       "          1.6088e+00, -1.8632e+00, -1.6864e+00, -9.4263e-01,  1.3499e+00,\n",
       "         -2.4072e-01, -2.7110e+00, -5.7922e-03, -4.9299e-01, -6.2298e-01,\n",
       "         -7.2571e-01,  7.1161e-02]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taking this one step further, we can have multiple neurons in the hidden layer\n",
    "\n",
    "W = torch.randn((27, 27)) \n",
    "# the first 27 is the number of input neurons\n",
    "# the second 27 is the number of neuron in the hidden layer\n",
    "logits = xenc @ W\n",
    "\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0032, 0.0072, 0.0129, 0.2039, 0.0367, 0.0086, 0.0852, 0.0791, 0.0500,\n",
       "         0.0509, 0.1113, 0.0094, 0.0374, 0.0155, 0.0125, 0.0107, 0.0019, 0.0071,\n",
       "         0.0053, 0.0208, 0.1643, 0.0088, 0.0176, 0.0088, 0.0113, 0.0146, 0.0050],\n",
       "        [0.0223, 0.1869, 0.0063, 0.0442, 0.0099, 0.1173, 0.0625, 0.0082, 0.0283,\n",
       "         0.0285, 0.0032, 0.0181, 0.0229, 0.0419, 0.0136, 0.0198, 0.0176, 0.0122,\n",
       "         0.2253, 0.0029, 0.0060, 0.0029, 0.0413, 0.0391, 0.0043, 0.0079, 0.0068],\n",
       "        [0.0127, 0.0135, 0.0105, 0.0095, 0.0846, 0.0026, 0.0060, 0.1585, 0.0930,\n",
       "         0.0280, 0.0107, 0.0295, 0.0681, 0.0107, 0.0083, 0.2174, 0.0098, 0.0254,\n",
       "         0.0285, 0.0580, 0.0311, 0.0066, 0.0087, 0.0263, 0.0066, 0.0204, 0.0150],\n",
       "        [0.0127, 0.0135, 0.0105, 0.0095, 0.0846, 0.0026, 0.0060, 0.1585, 0.0930,\n",
       "         0.0280, 0.0107, 0.0295, 0.0681, 0.0107, 0.0083, 0.2174, 0.0098, 0.0254,\n",
       "         0.0285, 0.0580, 0.0311, 0.0066, 0.0087, 0.0263, 0.0066, 0.0204, 0.0150],\n",
       "        [0.0087, 0.1203, 0.0687, 0.0095, 0.0333, 0.0087, 0.0813, 0.0160, 0.0448,\n",
       "         0.0084, 0.0630, 0.0178, 0.0462, 0.0303, 0.0835, 0.1270, 0.0039, 0.0047,\n",
       "         0.0099, 0.0981, 0.0200, 0.0017, 0.0253, 0.0155, 0.0136, 0.0123, 0.0273]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = logits.exp() # Exponentiate the log counts to get the counts\n",
    "probs = counts / counts.sum(1, keepdim=True) # Normalize the counts to get the probabilities\n",
    "probs # The probability of the next character given the current character. The calculation is known as the softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram:.e\n",
      "Input to NN:0\n",
      "Output probability predictions:tensor([0.0032, 0.0072, 0.0129, 0.2039, 0.0367, 0.0086, 0.0852, 0.0791, 0.0500,\n",
      "        0.0509, 0.1113, 0.0094, 0.0374, 0.0155, 0.0125, 0.0107, 0.0019, 0.0071,\n",
      "        0.0053, 0.0208, 0.1643, 0.0088, 0.0176, 0.0088, 0.0113, 0.0146, 0.0050])\n",
      "Acutal:e\n",
      "Probability assigned by the net to the correct charater:0.008610658347606659\n",
      "Negative log likelihood:4.754754543304443\n",
      "----\n",
      "\n",
      "Bigram:em\n",
      "Input to NN:5\n",
      "Output probability predictions:tensor([0.0223, 0.1869, 0.0063, 0.0442, 0.0099, 0.1173, 0.0625, 0.0082, 0.0283,\n",
      "        0.0285, 0.0032, 0.0181, 0.0229, 0.0419, 0.0136, 0.0198, 0.0176, 0.0122,\n",
      "        0.2253, 0.0029, 0.0060, 0.0029, 0.0413, 0.0391, 0.0043, 0.0079, 0.0068])\n",
      "Acutal:m\n",
      "Probability assigned by the net to the correct charater:0.04188360646367073\n",
      "Negative log likelihood:3.172860860824585\n",
      "----\n",
      "\n",
      "Bigram:mm\n",
      "Input to NN:13\n",
      "Output probability predictions:tensor([0.0127, 0.0135, 0.0105, 0.0095, 0.0846, 0.0026, 0.0060, 0.1585, 0.0930,\n",
      "        0.0280, 0.0107, 0.0295, 0.0681, 0.0107, 0.0083, 0.2174, 0.0098, 0.0254,\n",
      "        0.0285, 0.0580, 0.0311, 0.0066, 0.0087, 0.0263, 0.0066, 0.0204, 0.0150])\n",
      "Acutal:m\n",
      "Probability assigned by the net to the correct charater:0.010657386854290962\n",
      "Negative log likelihood:4.541501998901367\n",
      "----\n",
      "\n",
      "Bigram:ma\n",
      "Input to NN:13\n",
      "Output probability predictions:tensor([0.0127, 0.0135, 0.0105, 0.0095, 0.0846, 0.0026, 0.0060, 0.1585, 0.0930,\n",
      "        0.0280, 0.0107, 0.0295, 0.0681, 0.0107, 0.0083, 0.2174, 0.0098, 0.0254,\n",
      "        0.0285, 0.0580, 0.0311, 0.0066, 0.0087, 0.0263, 0.0066, 0.0204, 0.0150])\n",
      "Acutal:a\n",
      "Probability assigned by the net to the correct charater:0.013527781702578068\n",
      "Negative log likelihood:4.303009986877441\n",
      "----\n",
      "\n",
      "Bigram:a.\n",
      "Input to NN:1\n",
      "Output probability predictions:tensor([0.0087, 0.1203, 0.0687, 0.0095, 0.0333, 0.0087, 0.0813, 0.0160, 0.0448,\n",
      "        0.0084, 0.0630, 0.0178, 0.0462, 0.0303, 0.0835, 0.1270, 0.0039, 0.0047,\n",
      "        0.0099, 0.0981, 0.0200, 0.0017, 0.0253, 0.0155, 0.0136, 0.0123, 0.0273])\n",
      "Acutal:.\n",
      "Probability assigned by the net to the correct charater:0.00870790146291256\n",
      "Negative log likelihood:4.743524551391602\n",
      "----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# How the model is doing with random W\n",
    "\n",
    "l = len(words[0])+1\n",
    "nlls = torch.zeros(l)\n",
    "\n",
    "for i in range(l):\n",
    "    # i-th bigram\n",
    "    x=xs[i].item()\n",
    "    y=ys[i].item()\n",
    "\n",
    "    # The probability of the next character given the current character\n",
    "    print(f\"Bigram:{int_to_char[x]}{int_to_char[y]}\")\n",
    "    print(f\"Input to NN:{x}\")\n",
    "    print(f\"Output probability predictions:{probs[i]}\")\n",
    "    print(f\"Acutal:{int_to_char[y]}\")\n",
    "    p = probs[i, y]\n",
    "    print(f\"Probability assigned by the net to the correct charater:{p.item()}\")\n",
    "    nll = -torch.log(p).item()\n",
    "    print(f\"Negative log likelihood:{nll}\")\n",
    "    nlls[i] = nll\n",
    "    print(\"----\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = [],[]\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = char_to_int[ch1]\n",
    "        ix2 = char_to_int[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  228146\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num = xs.nelement()\n",
    "print('number of examples: ', num)\n",
    "\n",
    "# initialize the 'network'\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7686190605163574\n",
      "3.3788068294525146\n",
      "3.161090850830078\n",
      "3.027186155319214\n",
      "2.9344842433929443\n",
      "2.8672313690185547\n",
      "2.8166542053222656\n",
      "2.777146577835083\n",
      "2.745253801345825\n",
      "2.7188303470611572\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "for k in range(10):\n",
    "  \n",
    "  # forward pass\n",
    "  xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "  logits = xenc @ W # predict log-counts\n",
    "  counts = logits.exp() # counts, equivalent to N\n",
    "  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "  loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean() # Familiar with this? Regularization term!\n",
    "  print(loss.item())\n",
    "  \n",
    "  # backward pass\n",
    "  W.grad = None # set to zero the gradient\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13,  ..., 25, 26, 24])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dexzmalegllusailezktxha.\n",
      "kllimittain.\n",
      "lgdan.\n",
      "ka.\n",
      "da.\n"
     ]
    }
   ],
   "source": [
    "# finally, sample from the 'neural net' model\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "  \n",
    "  out = []\n",
    "  ix = 0\n",
    "  while True:\n",
    "    \n",
    "    # ----------\n",
    "    # BEFORE:\n",
    "    #p = P[ix]\n",
    "    # ----------\n",
    "    # NOW:\n",
    "    xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "    logits = xenc @ W # predict log-counts\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "    # ----------\n",
    "    \n",
    "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    out.append(int_to_char[ix])\n",
    "    if ix == 0:\n",
    "      break\n",
    "  print(''.join(out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "makemore-OzY8FFiS-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
