{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/isabellechen/git-repos/tutorial/makemore/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=open('names.txt','r').read().split()\n",
    "\n",
    "b={}\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        bigram = (ch1, ch2)\n",
    "        b[bigram]=b.get(bigram, 0) + 1\n",
    "        #print(ch1,ch2)\n",
    "\n",
    "chars = sorted(set(''.join(words)))\n",
    "char_to_int = {c: i+1 for i, c in enumerate(chars)}\n",
    "int_to_char = {i+1: c for i, c in enumerate(chars)}\n",
    "char_to_int['.']=0\n",
    "int_to_char[0]='.'\n",
    "N = torch.zeros((27,27), dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = [],[]\n",
    "\n",
    "for w in words[:1]:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = char_to_int[ch1]\n",
    "        ix2 = char_to_int[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13, 13,  1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys\n",
    "\n",
    "# For instance, When 5 (i.e. xs[1]) is the input, we want the probability of 13 (i.e. ys[1]) to be high!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hot encodings\n",
    "- We want integers, but we cannot put it directly into NN.\n",
    "- NN is made up of neurons, and neurons are made up of weights and biases.\n",
    "- We shouldn't multiply weights (and biases) by integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "xenc = F.one_hot(xs, num_classes=27)\n",
    "xenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1276a1210>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xenc, aspect='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cast the list of ints to list of floats\n",
    "xenc = xenc.float()\n",
    "xenc.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5475],\n",
       "        [-0.1884],\n",
       "        [-0.3580],\n",
       "        [-0.3580],\n",
       "        [ 0.4697]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RECALL: W dot x + b\n",
    "\n",
    "# Initialize W\n",
    "W = torch.randn((27, 1))\n",
    "xenc @ W # Matrix multiplication function, which produces the log counts\n",
    "\n",
    "# Think about this (5, 27) x (27, 1) = (5, 1): 5 activations for 5 inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.9134, -0.1550,  0.3498, -0.6385, -1.5682,  1.0636,  1.4040, -1.1097,\n",
       "         -0.3234, -1.1824,  1.4630,  0.6175,  1.1940, -1.7842, -1.7243, -1.0274,\n",
       "         -0.1001,  1.0985, -0.4474, -1.1233, -0.5143, -0.9258, -0.1222, -1.9806,\n",
       "         -0.9147,  1.3818,  0.1169],\n",
       "        [-0.4917, -0.5197,  0.6692, -0.0246,  1.0056,  0.0755,  1.1588, -3.1326,\n",
       "         -0.6301,  0.8637, -1.5456,  1.9804,  1.0016, -0.5666,  0.8197,  0.2507,\n",
       "          0.7124,  0.3802,  0.9057,  1.9518, -1.4652,  0.6457,  0.0866, -0.7712,\n",
       "         -0.4761, -2.0561,  0.4146],\n",
       "        [ 0.8517, -1.3331,  0.5173,  0.9534,  0.6218, -0.6049,  0.4501,  0.2164,\n",
       "          0.4663,  0.5125, -0.6241, -0.6393,  0.6493, -1.2030, -1.7165,  1.1670,\n",
       "          0.3596,  0.4164, -1.5382, -1.7832, -1.7356,  0.1162, -0.8528, -0.4332,\n",
       "          0.3341,  0.3309,  1.1526],\n",
       "        [ 0.8517, -1.3331,  0.5173,  0.9534,  0.6218, -0.6049,  0.4501,  0.2164,\n",
       "          0.4663,  0.5125, -0.6241, -0.6393,  0.6493, -1.2030, -1.7165,  1.1670,\n",
       "          0.3596,  0.4164, -1.5382, -1.7832, -1.7356,  0.1162, -0.8528, -0.4332,\n",
       "          0.3341,  0.3309,  1.1526],\n",
       "        [ 1.1052,  2.3255, -1.1579,  1.5809,  1.1068, -0.1464,  0.1287, -0.7900,\n",
       "         -0.0113,  0.4064, -1.2089,  0.2678, -1.9230, -0.4066, -0.8345, -2.4865,\n",
       "          0.7148,  0.4175,  0.7377,  1.2828,  1.9681, -1.3656,  0.4040,  1.9116,\n",
       "          1.2284,  0.5725, -0.5367]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taking this one step further, we can have multiple neurons in the hidden layer\n",
    "\n",
    "W = torch.randn((27, 27)) \n",
    "# the first 27 is the number of input neurons\n",
    "# the second 27 is the number of neuron in the hidden layer\n",
    "logits = xenc @ W\n",
    "\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0043, 0.0251, 0.0417, 0.0155, 0.0061, 0.0851, 0.1195, 0.0097, 0.0212,\n",
       "         0.0090, 0.1268, 0.0544, 0.0969, 0.0049, 0.0052, 0.0105, 0.0266, 0.0881,\n",
       "         0.0188, 0.0095, 0.0176, 0.0116, 0.0260, 0.0041, 0.0118, 0.1169, 0.0330],\n",
       "        [0.0129, 0.0126, 0.0412, 0.0206, 0.0577, 0.0228, 0.0673, 0.0009, 0.0112,\n",
       "         0.0501, 0.0045, 0.1530, 0.0575, 0.0120, 0.0479, 0.0271, 0.0431, 0.0309,\n",
       "         0.0522, 0.1487, 0.0049, 0.0403, 0.0230, 0.0098, 0.0131, 0.0027, 0.0320],\n",
       "        [0.0696, 0.0078, 0.0498, 0.0770, 0.0553, 0.0162, 0.0466, 0.0369, 0.0473,\n",
       "         0.0496, 0.0159, 0.0157, 0.0568, 0.0089, 0.0053, 0.0953, 0.0425, 0.0450,\n",
       "         0.0064, 0.0050, 0.0052, 0.0333, 0.0127, 0.0192, 0.0415, 0.0413, 0.0940],\n",
       "        [0.0696, 0.0078, 0.0498, 0.0770, 0.0553, 0.0162, 0.0466, 0.0369, 0.0473,\n",
       "         0.0496, 0.0159, 0.0157, 0.0568, 0.0089, 0.0053, 0.0953, 0.0425, 0.0450,\n",
       "         0.0064, 0.0050, 0.0052, 0.0333, 0.0127, 0.0192, 0.0415, 0.0413, 0.0940],\n",
       "        [0.0503, 0.1704, 0.0052, 0.0809, 0.0504, 0.0144, 0.0189, 0.0076, 0.0165,\n",
       "         0.0250, 0.0050, 0.0218, 0.0024, 0.0111, 0.0072, 0.0014, 0.0340, 0.0253,\n",
       "         0.0348, 0.0601, 0.1192, 0.0043, 0.0249, 0.1127, 0.0569, 0.0295, 0.0097]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = logits.exp() # Exponentiate the log counts to get the counts\n",
    "probs = counts / counts.sum(1, keepdim=True) # Normalize the counts to get the probabilities\n",
    "probs # The probability of the next character given the current character. The calculation is known as the softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram:.e\n",
      "Predicted:tensor([0.0043, 0.0251, 0.0417, 0.0155, 0.0061, 0.0851, 0.1195, 0.0097, 0.0212,\n",
      "        0.0090, 0.1268, 0.0544, 0.0969, 0.0049, 0.0052, 0.0105, 0.0266, 0.0881,\n",
      "        0.0188, 0.0095, 0.0176, 0.0116, 0.0260, 0.0041, 0.0118, 0.1169, 0.0330])\n",
      "Acutal:e\n",
      "Probability assigned by the net to the correct charater:0.08505567163228989\n",
      "Negative log likelihood:2.464449167251587\n",
      "----\n",
      "\n",
      "Bigram:em\n",
      "Predicted:tensor([0.0129, 0.0126, 0.0412, 0.0206, 0.0577, 0.0228, 0.0673, 0.0009, 0.0112,\n",
      "        0.0501, 0.0045, 0.1530, 0.0575, 0.0120, 0.0479, 0.0271, 0.0431, 0.0309,\n",
      "        0.0522, 0.1487, 0.0049, 0.0403, 0.0230, 0.0098, 0.0131, 0.0027, 0.0320])\n",
      "Acutal:m\n",
      "Probability assigned by the net to the correct charater:0.011982955038547516\n",
      "Negative log likelihood:4.424270153045654\n",
      "----\n",
      "\n",
      "Bigram:mm\n",
      "Predicted:tensor([0.0696, 0.0078, 0.0498, 0.0770, 0.0553, 0.0162, 0.0466, 0.0369, 0.0473,\n",
      "        0.0496, 0.0159, 0.0157, 0.0568, 0.0089, 0.0053, 0.0953, 0.0425, 0.0450,\n",
      "        0.0064, 0.0050, 0.0052, 0.0333, 0.0127, 0.0192, 0.0415, 0.0413, 0.0940])\n",
      "Acutal:m\n",
      "Probability assigned by the net to the correct charater:0.008913007564842701\n",
      "Negative log likelihood:4.720243453979492\n",
      "----\n",
      "\n",
      "Bigram:ma\n",
      "Predicted:tensor([0.0696, 0.0078, 0.0498, 0.0770, 0.0553, 0.0162, 0.0466, 0.0369, 0.0473,\n",
      "        0.0496, 0.0159, 0.0157, 0.0568, 0.0089, 0.0053, 0.0953, 0.0425, 0.0450,\n",
      "        0.0064, 0.0050, 0.0052, 0.0333, 0.0127, 0.0192, 0.0415, 0.0413, 0.0940])\n",
      "Acutal:a\n",
      "Probability assigned by the net to the correct charater:0.00782529916614294\n",
      "Negative log likelihood:4.850393295288086\n",
      "----\n",
      "\n",
      "Bigram:a.\n",
      "Predicted:tensor([0.0503, 0.1704, 0.0052, 0.0809, 0.0504, 0.0144, 0.0189, 0.0076, 0.0165,\n",
      "        0.0250, 0.0050, 0.0218, 0.0024, 0.0111, 0.0072, 0.0014, 0.0340, 0.0253,\n",
      "        0.0348, 0.0601, 0.1192, 0.0043, 0.0249, 0.1127, 0.0569, 0.0295, 0.0097])\n",
      "Acutal:.\n",
      "Probability assigned by the net to the correct charater:0.05029982328414917\n",
      "Negative log likelihood:2.9897537231445312\n",
      "----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# How the model is doing with random W\n",
    "\n",
    "l = len(words[0])+1\n",
    "nlls = torch.zeros(l)\n",
    "\n",
    "for i in range(l):\n",
    "    # i-th bigram\n",
    "    x=xs[i].item()\n",
    "    y=ys[i].item()\n",
    "\n",
    "    # The probability of the next character given the current character\n",
    "    print(f\"Bigram:{int_to_char[x]}{int_to_char[y]}\")\n",
    "    print(f\"Predicted:{probs[i]}\")\n",
    "    print(f\"Acutal:{int_to_char[y]}\")\n",
    "    p = probs[i, y]\n",
    "    print(f\"Probability assigned by the net to the correct charater:{p.item()}\")\n",
    "    nll = -torch.log(p).item()\n",
    "    print(f\"Negative log likelihood:{nll}\")\n",
    "    nlls[i] = nll\n",
    "    print(\"----\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "makemore-OzY8FFiS-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
